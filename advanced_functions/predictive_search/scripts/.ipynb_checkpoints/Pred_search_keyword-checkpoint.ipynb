{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module for predicting top n best possible word matches given a word\n",
    "\n",
    "# Context:\n",
    "# A -> True word\n",
    "# B -> Typed word\n",
    "\n",
    "# Make dictionary, key (true word), value (list of incorrect words)\n",
    "# tru_in dict is A -> [B1,B2,...,Bn]\n",
    "missp_dat_path = \"/Users/daveistanto/Documents/project_normal/advanced_functions/predictive_search/dataset/missp.dat\"\n",
    "wiki_dat_path = \"/Users/daveistanto/Documents/project_normal/advanced_functions/predictive_search/dataset/wikipedia.dat\"\n",
    "aspell_dat_path = \"/Users/daveistanto/Documents/project_normal/advanced_functions/predictive_search/dataset/aspell.dat\"\n",
    "\n",
    "tru_in = dict()\n",
    "\n",
    "def add_to_dict(data_path, input_dict):\n",
    "    with open(data_path, 'r') as df:\n",
    "        data_lines = df.readlines()\n",
    "        for line in data_lines:\n",
    "            lns = line[:-1].upper()\n",
    "            if lns[0] == \"$\":     \n",
    "                curr_key = lns[1:]\n",
    "                if curr_key not in list(set(input_dict)):\n",
    "                    input_dict[curr_key] = []\n",
    "            else:\n",
    "                 input_dict[curr_key].append(lns)\n",
    "    \n",
    "    for k, v in input_dict.items():\n",
    "        tru_in[k] = list(set(tru_in[k]))\n",
    "    return input_dict\n",
    "\n",
    "tru_in = add_to_dict(missp_dat_path, tru_in)\n",
    "tru_in = add_to_dict(wiki_dat_path, tru_in)\n",
    "tru_in = add_to_dict(aspell_dat_path, tru_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get P(B|A) distribution for specific B. Does not equal 1 because different A(s).\n",
    "\n",
    "def get_P_BA(B):\n",
    "    PBA_dict = dict()\n",
    "    B = B.upper()\n",
    "    for k, v in tru_in.items():\n",
    "        PBA_for_given_A = tru_in[k].count(B) / len(tru_in[k])\n",
    "        PBA_dict[k] = PBA_for_given_A\n",
    "        \n",
    "    return PBA_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRE\n"
     ]
    }
   ],
   "source": [
    "# Get P(A) distribution\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "word_freq_path = \"/Users/daveistanto/Documents/project_normal/advanced_functions/predictive_search/dataset/top_5k_words.csv\"\n",
    "word_freq_df = pd.read_csv(word_freq_path)\n",
    "\n",
    "def normalize_word(input_word):\n",
    "    input_word = input_word.upper()[3:]\n",
    "    return input_word\n",
    "\n",
    "normalize_word = np.vectorize(normalize_word)\n",
    "\n",
    "word_freq_df[\"Normalized word\"] = np.apply_along_axis(normalize_word, 0, word_freq_df[\"Word\"].values)\n",
    "word_freq_df = word_freq_df.drop(columns=[\"Word\"])\n",
    "word_freq_df[\"Relative Frequency\"] = 0\n",
    "\n",
    "# Add dummy frequency of 1000 if word not found\n",
    "for k in tru_in.keys():\n",
    "    if k.upper() not in word_freq_df[\"Normalized word\"].values:\n",
    "        new_row = {\"Frequency\": 1000, \"Normalized word\": k}\n",
    "        word_freq_df = word_freq_df.append(new_row, ignore_index=True)\n",
    "\n",
    "# Add relative frequency (P(A)) for each A\n",
    "word_sum = word_freq_df[\"Frequency\"].sum()\n",
    "for row_index in range(len(word_freq_df)):\n",
    "    word_freq_df.iloc[row_index, 2] = word_freq_df.iloc[row_index, 0] / word_sum\n",
    "    \n",
    "    \n",
    "# Sort word_freq_df by frequency\n",
    "word_freq_df = word_freq_df.sort_values(by=\"Frequency\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get P(A) given specific A:\n",
    "\n",
    "def get_PA(A):\n",
    "    P_A = word_freq_df[\"Relative Frequency\"][word_freq_df[\"Normalized word\"] == A].values[0]\n",
    "    return P_A\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ARE', 1), ('A', 0.004335377292933662), ('AND', 0.0032133261164172633), ('I', 0.0009917897109013784), ('OR', 0.000515801002713679), ('AREA', 0.00024802365183412275), ('AIR', 7.922720154781406e-05), ('OUR', 4.909134884072801e-05), ('EYE', 1.94628210576491e-05), ('FIRE', 4.935017633029245e-06), ('ERE', 6.79914744173913e-08)]\n"
     ]
    }
   ],
   "source": [
    "# Get relative P(A|B)\n",
    "\n",
    "def get_PAB(B):\n",
    "    B = B.upper()\n",
    "    PBA_dict = get_P_BA(B)\n",
    "    PAB_list = []\n",
    "    for k, v in PBA_dict.items():\n",
    "            if v != 0:\n",
    "                P_A = get_PA(k)\n",
    "                tup = (k, PBA_dict[k] * P_A)\n",
    "                PAB_list.append(tup)\n",
    "\n",
    "    PAB_list.append((B, 1))\n",
    "    \n",
    "    # Sort by value, descending\n",
    "    PAB_list.sort(key=lambda tup: tup[1], reverse=True)\n",
    "    return PAB_list\n",
    "\n",
    "print(get_PAB(\"are\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
